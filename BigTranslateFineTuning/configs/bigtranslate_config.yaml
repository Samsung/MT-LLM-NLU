model:
    name: James-WYang/BigTranslate
    output_dir: BigTranslateSlotTranslator
    
data:
    DATA_PATH: massive
    train_language_pairs: [en-es,en-de,en-fr,en-pt,en-tr,en-zh,en-ja,pt-en,es-en,de-en,fr-en,tr-en,zh-en,ja-en]
    valid_language_pairs: [en-es,en-de,en-fr,en-pt,en-tr,en-zh,en-ja]

training:
    output_dir: bigtranslate_slot_translator_ckpt
    
    num_train_epochs: 1
    optim: adamw_torch
    weight_decay : 0.01
    learning_rate: 0.00025
    
    bf16: true
    gradient_accumulation_steps: 1
    per_device_train_batch_size: 3
    per_device_eval_batch_size: 3
    
    do_train: true
    do_eval: true
   
    save_steps: .5
    eval_steps: .5
    logging_steps: .02
    
    metric_for_best_model: "eval_loss"
    greater_is_better: false
    
    report_to: []
    ddp_timeout: 99999
    overwrite_output_dir: true
    
    logging_strategy: steps
    evaluation_strategy: steps
    
    load_best_model_at_end: true

LORA:
    rank: 16
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
    dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"